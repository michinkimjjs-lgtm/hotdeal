import logging
import sys
import os
import time
from crawler import PpomppuCrawler, FMKoreaCrawler, RuliwebCrawler

# Configure Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("crawler")

def main():
    logger.info("ğŸš€ [Cloud Runner] GitHub Actions í¬ë¡¤ë§ ì‹œì‘")

    # Load Secrets from Environment Variables (GitHub Secrets)
    SUPABASE_URL = os.environ.get("SUPABASE_URL")
    SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

    if not SUPABASE_URL:
        logger.error("âŒ [ì˜¤ë¥˜] SUPABASE_URLì´ ì—†ìŠµë‹ˆë‹¤. Settings > Secretsì— ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    if not SUPABASE_KEY:
        logger.error("âŒ [ì˜¤ë¥˜] SUPABASE_KEYê°€ ì—†ìŠµë‹ˆë‹¤. Settings > Secretsì— ë“±ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    if not SUPABASE_URL or not SUPABASE_KEY:
        sys.exit(1)

    try:
        # Initialize Crawlers
        logger.info("ğŸ› ï¸ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì¤‘...")
        ppomppu = PpomppuCrawler(SUPABASE_URL, SUPABASE_KEY)
        fmkorea = FMKoreaCrawler(SUPABASE_URL, SUPABASE_KEY)
        ruliweb = RuliwebCrawler(SUPABASE_URL, SUPABASE_KEY)

        # Run Crawlers
        logger.info("--- ë½ë¿Œ í¬ë¡¤ë§ ì‹œì‘ ---")
        ppomppu.crawl(limit=10) # 1íšŒ ì‹¤í–‰ ì‹œ ìµœì‹  10ê°œë§Œ í™•ì¸ (íš¨ìœ¨ì„±)
        
        logger.info("--- í¨ì½” í¬ë¡¤ë§ ì‹œì‘ ---")
        fmkorea.crawl(limit=10)

        logger.info("--- ë£¨ë¦¬ì›¹ í¬ë¡¤ë§ ì‹œì‘ ---")
        ruliweb.crawl(limit=10)
        
        logger.info("âœ… ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        sys.exit(1)
    finally:
        # Ensure browsers are closed if they were opened
        if 'fmkorea' in locals():
            fmkorea.stop_browser()

if __name__ == "__main__":
    main()
