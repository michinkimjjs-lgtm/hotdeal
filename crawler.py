import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
import time
import re
import random

# 1. Supabase ì„¤ì • (ë³¸ì¸ì˜ ì •ë³´ë¡œ ë³€ê²½ í•„ìš”)
URL: str = "https://zvlntvovzffizoruwxqx.supabase.co"
KEY: str = "sb_publishable_QQaxPklEyj2C7IVhtmspMg_AQmHkQKV"

class HotDealCrawler:
    def __init__(self, supabase_url, supabase_key):
        """
        ì´ˆê¸°í™”: Supabase í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        self.supabase: Client = create_client(supabase_url, supabase_key)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

    def fetch_page(self, url, retries=3):
        """
        ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ í˜ì´ì§€ ìš”ì²­ í•¨ìˆ˜
        """
        for i in range(retries):
            try:
                response = requests.get(url, headers=self.headers, timeout=10)
                if response.status_code == 200:
                    return response.text
                print(f"[{i+1}/{retries}] í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {response.status_code}")
            except Exception as e:
                print(f"[{i+1}/{retries}] ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë°œìƒ: {e}")
            
            if i < retries - 1:
                wait_time = (i + 1) * 2 + random.uniform(0, 1)
                print(f"{wait_time:.1f}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(wait_time)
        return None

    def extract_price(self, title):
        """
        ê²Œì‹œê¸€ ì œëª©ì—ì„œ ê°€ê²© ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        # 1. ê´„í˜¸ ì•ˆì˜ ê°€ê²© ì •ë³´ (ì˜ˆ: (69,000/ë¬´ë£Œ) ë“±)
        match = re.search(r'[\(\[]\s*([\d,]+(?:ì›|ë§Œì›|ì›)?)\s*(?:/|\]|\))', title)
        if match:
            return match.group(1).strip()
        
        # 2. 'ì›' ë˜ëŠ” 'ë§Œì›' í‚¤ì›Œë“œ ì•ì˜ ìˆ«ì
        match_won = re.search(r'([\d,]+(?:ì›|ë§Œì›))', title)
        if match_won:
            return match_won.group(1).strip()
            
        return "ê°€ê²©ë¯¸ìƒ"

    def crawl_ppomppu(self):
        """
        ë½ë¿Œ ê²Œì‹œíŒì„ í¬ë¡¤ë§í•˜ê³  Supabaseì— ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        """
        print("\nğŸš€ [Ppomppu] í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        target_url = "https://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu"
        html = self.fetch_page(target_url)
        
        if not html:
            print("âŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í•˜ì—¬ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return

        soup = BeautifulSoup(html, 'html.parser')
        # ê²Œì‹œê¸€ ëª©ë¡ ì„ íƒ (ë½ë¿Œ íŠ¹ìœ ì˜ í´ë˜ìŠ¤ëª…)
        items = soup.select('tr.common-list0, tr.common-list1')
        
        success_count = 0
        error_count = 0

        for item in items:
            try:
                # ì œëª© ìš”ì†Œ ì°¾ê¸°
                title_el = item.select_one('font.list_title')
                if not title_el:
                    continue
                
                full_title = title_el.get_text().strip()
                # ë½ë¿Œ ë§í¬ëŠ” ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                link_el = item.select_one('td:nth-child(3) > a')
                if not link_el:
                    continue
                link = "https://www.ppomppu.co.kr/zboard/" + link_el['href']
                
                # ì¸ë„¤ì¼
                img_el = item.select_one('.thumb_border')
                img_url = "https:" + img_el['src'] if img_el else ""
                
                # ê°€ê²© ì¶”ì¶œ
                price = self.extract_price(full_title)
                
                # ë°ì´í„° êµ¬ì„± (Supabase í…Œì´ë¸” ì»¬ëŸ¼ê³¼ ì¼ì¹˜)
                data = {
                    "title": full_title,
                    "url": link,
                    "img_url": img_url,
                    "source": "Ppomppu",
                    "category": "ê¸°íƒ€",
                    "price": price
                }
                
                # Supabase Upsert (urlì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬)
                res = self.supabase.table("hotdeals").upsert(data, on_conflict="url").execute()
                
                if hasattr(res, 'data') and len(res.data) > 0:
                    print(f"âœ… ì €ì¥ ì„±ê³µ: {full_title[:25]}... [{price}]")
                    success_count += 1
                else:
                    # upsertì˜ ê²½ìš° ë°ì´í„°ê°€ ë³€í•˜ì§€ ì•Šìœ¼ë©´ res.dataê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìŒ
                    print(f"â„¹ï¸ ì—…ë°ì´íŠ¸ë¨(ë˜ëŠ” ë³€í™”ì—†ìŒ): {full_title[:25]}...")
                    success_count += 1
                    
            except Exception as e:
                print(f"âŒ ê°œë³„ í•­ëª© ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                error_count += 1

        print(f"\nâœ¨ í¬ë¡¤ë§ ì™„ë£Œ! (ì„±ê³µ: {success_count}, ì—ëŸ¬: {error_count})")

if __name__ == "__main__":
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = HotDealCrawler(URL, KEY)
    crawler.crawl_ppomppu()
